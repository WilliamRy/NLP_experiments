# I GLUE (General Language Understanding Evaluation)

1. MNLI multi-genre natual language inference

+ Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or
neutral with respect to the first one.

2. QQP quora question pairs

+ determine if two questions asked on quora are semantically equivalent

3.QNLI question natural language inference 

+ the positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples do not.

4. SST-2 Stanford Sentiment Treebank

+ a binary single-sentence classification task consisting of sentences extracted from movie reviews

5. CoLA The Corpus of Linguistic Acceptability

+ a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically "acceptable" or not.

6. STS-B The Semantic Textual Similarity Benchmark 

+ a collection of sentence pairs drawn from news headlines and other sources. They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.

7. MRPC Microsoft Research Paraphrase Corpus

+ sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.

8. RTE Recognizing Textual Entailment

+ a binary entailment task similar to MNLI, but with much less training data.

# II and more ...

9. SQuAD Standford Question Answering Dataset

+ a collection of 100k crowdsourced question/answer pairs. Given a question and a paragraph from Wikipedia containing the answer, the task is to predict the answer text span in the paragraph.

10. CoNLL 2003 NER dataset (Got)

+ consists of 200k training words which have been annotated as Person, Organization, Location, Miscellaneous or other.

11. SWAG The Situations With Adversarial Generations

+ 113k sentence-pair completion examples that evaluate grounded commonsense inference.






## References

+ Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
+ https://github.com/google-research/bert#fine-tuning-with-bert



